## Overview

This project implements and optimizes machine learning models for classification using various algorithms, including Random Forest, K-Nearest Neighbors (KNN), and LightGBM (Gradient Boosting). It also addresses class imbalance with SMOTE (Synthetic Minority Over-sampling Technique) and tunes hyperparameters using GridSearchCV.

### Requirements

- Python 3.x
- `pandas`: For data manipulation and analysis.
- `numpy`: For numerical operations.
- `scikit-learn`: For machine learning models, evaluation, and cross-validation.
- `imbalanced-learn`: For SMOTE, a technique for handling class imbalance.
- `lightgbm`: For training the LightGBM gradient boosting model.
- `joblib`: For parallel processing during hyperparameter tuning with GridSearchCV.

To install the necessary libraries, you can use pip:

```bash
pip install pandas numpy scikit-learn imbalanced-learn lightgbm joblib
```

### Dataset

The dataset is assumed to be in CSV format, where:

- **id**: A unique identifier for each sample (not used for modeling).
- **target**: The target variable to predict.
- **Features**: The input features used for model training.

### File Structure

- **main.py**: The Python script containing the code for training and evaluating the models.
- **data.csv**: The input dataset in CSV format.

### Script Explanation

1. **Data Loading and Preprocessing**:
    - The dataset is loaded from a CSV file.
    - Missing values are filled using the median of the respective columns.
    - Features are standardized using `StandardScaler` to have a mean of 0 and variance of 1.
2. **Class Imbalance Handling**:
    - SMOTE (Synthetic Minority Over-sampling Technique) is applied to oversample the minority class and balance the dataset.
3. **Model Optimization**:
    - Three machine learning models (Random Forest, KNN, and LightGBM) are optimized using GridSearchCV with cross-validation.
    - Hyperparameters for each model are fine-tuned to improve F1-score.
4. **Model Evaluation**:
    - After training, the models are evaluated using the F1-score metric (macro average).
    - The F1-score is printed for each model to assess its performance.

### Usage

To run the script, simply execute the Python file:

```bash
python main.py
```

### Notes

- The script assumes that the dataset file `data.csv` is available in the same directory.
- The script uses a temporary folder (`D:/Temp/joblib`) for storing intermediate files generated by joblib. You may need to adjust this path based on your system.
- The ensemble model part (VotingClassifier) is currently commented out but can be enabled for further improvements.